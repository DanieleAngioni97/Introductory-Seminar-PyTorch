{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4",
   "authorship_tag": "ABX9TyMJ/kpnjukDCGFKTpNSvuRX",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DanieleAngioni97/Introductory-Seminar-PyTorch/blob/main/notebooks/01_tensor_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Basics"
   ],
   "metadata": {
    "id": "N82uAFMBXNCe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we will see what is a tensor object and how we can manipulate this particular data structure."
   ],
   "metadata": {
    "id": "nafDjpzAYEhY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's import some libraries that we will need later."
   ],
   "metadata": {
    "id": "d3sBWFdwXPn7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "tUpZ4eWGO3Oo"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do you remember the python list? This is a basic data structure in which we can collect data in an organized manner.\n",
    "For example, we can create a list of integers and obtain the values by using the correponding indices (remember that we start counting from 0)"
   ],
   "metadata": {
    "id": "k_u9liplXTsY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fND6dEogrrC0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d430113-973b-4d04-feb4-59b198336e6b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "l = [1, 0, 3]\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also create a list of lists, so that we can simulate a matrix-like data structure.\n",
    "For example, we can create a 2x3 matrix (2 rows and 3 columns) by creating a list of 2 list with 3 elements each.\n",
    "Here we can obtain each element by first selecting the row and then the column."
   ],
   "metadata": {
    "id": "26athsA9YCBO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nested_list = [[0, 1, 2],\n",
    "               [1, 2, 3]]\n",
    "print(nested_list[1][2])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKHXCWGAvPXJ",
    "outputId": "cfb76297-e934-441f-f719-99b2be4160c0"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the exact same concept we can create a nested list and pass it as argument to torch.tensor() in order to create a PyTorch tensor data structure.\n",
    "Now we can perform indexing in a more matrix-like fashion.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "mgmR7sphY-ck"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "t = torch.tensor([[0, 1, 2],\n",
    "                  [1, 2, 3]])\n",
    "print(t[0, 1]) # indexing tensors (we will see more indexing tricks later)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6Bok9vMvYWi",
    "outputId": "4a5f93a6-5a37-4b08-bf65-0bd51585bdb5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fancy Indexing\n",
    "\n",
    "With tensors, we can use fancy indexing (like [Numpy indexing](https://numpy.org/doc/stable/user/basics.indexing.html))"
   ],
   "metadata": {
    "id": "lyN3T2SXvn6C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([0, 1, 2, 3, 4, 5, 6]) # 1-d tensor\n",
    "element = x[0] # i-th element\n",
    "first_elements = x[:3] # from start to element 3\n",
    "last_elements = x[3:] # from element 3 to the end\n",
    "some_elements = x[3:5] # from element 1 to element 3"
   ],
   "metadata": {
    "id": "qlPrC9nDvnBg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "When tensors are too large it can be difficult to keep track of the last indices, but there is a trick ;)"
   ],
   "metadata": {
    "id": "FCAv4YW1w2PC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.arange(100) # 1-d tensor\n",
    "\n",
    "last_element = x[99]\n",
    "\n",
    "# A clever alternative\n",
    "last_element = x[-1]\n",
    "\n",
    "third_to_last_elements = x[-3:] # the last three elements\n",
    "\n",
    "# Some combinations... what should it print?\n",
    "some_elements = x[90: -5]\n",
    "some_elements = x[-10: -5]"
   ],
   "metadata": {
    "id": "drwF-OAew4iO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Works similarly with 2-d tensors (row and columns):"
   ],
   "metadata": {
    "id": "AN9YFZQkv6Zx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[0, 1, 2], [1, 2, 3]]) # 2-d tensor\n",
    "element = x[0, 0]\n",
    "row = x[0, :] # works also with x[0] in this case\n",
    "column = x [:, 0]\n",
    "some_rows = x[1:, :] # from row 1 to the end, all columns\n",
    "some_elements = x[1:2, :1] # from row 1 to 2, from column 0 to 1"
   ],
   "metadata": {
    "id": "EI1p9PVwv6I6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors element types"
   ],
   "metadata": {
    "id": "rcphiS3SyxhU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "double_precision = torch.tensor([0, 1], dtype=torch.double)\n",
    "print(double_precision.dtype)\n",
    "short_tensor = double_precision.short()\n",
    "print(short_tensor.dtype)\n",
    "bool_tensor = double_precision.bool()\n",
    "print(bool_tensor.dtype)"
   ],
   "metadata": {
    "id": "bRLoNaZRyc1T",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f39b3cfb-a059-4682-b28f-f33f725e9634"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.float64\n",
      "torch.int16\n",
      "torch.bool\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general you can call the `.type` method and specify the torch data type (a complete list in the [documentation](https://pytorch.org/docs/stable/tensor_attributes.html))"
   ],
   "metadata": {
    "id": "bjvGzE3M_Dwr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([0, 1], dtype=torch.double)\n",
    "x.type(torch.uint8)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kebY2NS1-Yfx",
    "outputId": "443bab19-9173-405b-cc0b-77fc6bc54590"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 1], dtype=torch.uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Boolean indexing\n",
    "Similarly to NumPy, we can use the boolean tensors to indicise certain elements of another tensor."
   ],
   "metadata": {
    "id": "LM-tW9e6joRP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[-4, -1, 2], [1, -2, 3]]) # 2-d tensor\n",
    "\n",
    "boolean_mask = (x > 0)  # this could be any boolean expression\n",
    "print(boolean_mask)\n",
    "print(x[boolean_mask])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tf5cqmZtj5ee",
    "outputId": "def1261d-4dff-4271-af8b-36a760c9f393"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[False, False,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([2, 1, 3])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic tensor operations"
   ],
   "metadata": {
    "id": "WbahSTgDzEJd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "a = torch.ones(3, 2) # 3x2 tensor of only ones\n",
    "b = torch.zeros(3, 1) # 3x1 tensor of only zeros\n",
    "c = torch.zeros_like(a) # same shape and type as a\n",
    "a_t = a.t() # 2x3 tensor (transpose of a)\n",
    "print(a.shape) # prints the shape (i.e., all the sizes of the dimensions)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZhbLY-eAtne",
    "outputId": "195e6e76-6c14-4f3a-ef4a-799ac2aad13d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "absolute_values = torch.abs(a) # pointwise operations\n",
    "mean_value = torch.mean(a) # reduction operations\n",
    "s = a + c # element-wise sum\n",
    "p = a * c # element-wise product\n",
    "z = torch.mm(a, c.t()) # matrix multiplication (careful with shapes!)\n",
    "broadcasting = a + torch.tensor([1, 2]) # torch tries to match shapes"
   ],
   "metadata": {
    "id": "uakeXB3AzGSd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Tensor storage](https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html#torch-tensor-data-ptr)\n"
   ],
   "metadata": {
    "id": "rsgYxtMI0ASz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = a[1] # different Tensor, same storage (points to the same location)\n",
    "c = a.reshape([2, 2]) # same storage, different stride\n",
    "print(a.storage())\n",
    "print(c.storage())\n",
    "print(a.data_ptr() == c.data_ptr()) # same storage\n",
    "print(c.stride()) # how many storage items to skip for incrementing each dimension"
   ],
   "metadata": {
    "id": "IAG5eB1V0CbY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember: the underlying memory is allocated only once, which makes the view\n",
    "operation very lightweigth even for large storages."
   ],
   "metadata": {
    "id": "EDn6cw7H0N7H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modifying stored values: in-place operations\n",
    "\n",
    "In-place operations are used to modify directly stored values. The most used one\n",
    "is the zero_, that sets to zero all values. They can be recognized by the trailing\n",
    "underscore _ in their name."
   ],
   "metadata": {
    "id": "RokTdtquBz1I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "a = torch.ones(3, 2)\n",
    "a.zero_() # in-place operation, does not create a new tensor"
   ],
   "metadata": {
    "id": "5HsziIQOB3wx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Moving tensors to the GPU"
   ],
   "metadata": {
    "id": "e6VeiFQ30XFP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "gpu_tensor = torch.zeros(1, device='cuda') # created on the GPU\n",
    "cpu_tensor = torch.zeros(1)\n",
    "to_gpu = cpu_tensor.to(device='cuda') # this creates a copy of the tensor!\n",
    "to_gpu_another = cpu_tensor.cuda() # shorthand for the previous command\n",
    "again_to_cpu = to_gpu.cpu() # shorthand for copying the tensor to cpu"
   ],
   "metadata": {
    "id": "CVf3iDyj0Ybq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Serializing tensors"
   ],
   "metadata": {
    "id": "UG1eVhb30jzn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(a, 'tensor.pth') # note that the extension is arbitrary"
   ],
   "metadata": {
    "id": "YFUkCWSr0mSH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "b = torch.load('tensor.pth')"
   ],
   "metadata": {
    "id": "RKQqtidOG58O"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.1\n",
    "\n",
    "Write code that creates a `torch.tensor` with the following contents:\n",
    "$\\begin{bmatrix}\n",
    "1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\\\ 9 & 0 & -1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Now with this tensor:\n",
    "1. Print the second element in the second row\n",
    "2. Print the last column\n",
    "3. Print the shape of the tensor\n",
    "4. Compute the average of each row\n",
    "5. Print the tensor of all positive values and save it (*hint: use boolean indexing*)\n",
    "6. Load the latter tensor and compute its average\n",
    "7. Move the original tensor to the GPU"
   ],
   "metadata": {
    "id": "FE0JrsY5Byzl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO 1)"
   ],
   "metadata": {
    "id": "IjFHCEoF8rJH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO 2)"
   ],
   "metadata": {
    "id": "yf-kqJyA9Csy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO 3)"
   ],
   "metadata": {
    "id": "0aoOnBLS9Dq3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO 4)"
   ],
   "metadata": {
    "id": "rLLn-KuJ9EyL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO 5)"
   ],
   "metadata": {
    "id": "lOc7_ziQ9FiE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO 6)"
   ],
   "metadata": {
    "id": "Hha2a7lU9GTZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO 7)"
   ],
   "metadata": {
    "id": "4JY2zLBc9HND"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autograd\n",
    "As we will see in the next chapters, in deep learning we need to obtain the **gradients**.\n",
    "PyTorch computes the gradient of any differentiable function w.r.t. their inputs by using **automatic differentiation** (even for extremely complex functions!)\n",
    "This PyTorch component is called **autograd**.\n",
    "\n",
    "When operating with tensors, PyTorch automatically build the corresponding computational graph by keeping track of every interaction between tensors.\n",
    "In particular each node remembers:\n",
    "*   the parent tensors that originated it\n",
    "*   the operation performed on the parent tensors\n",
    "\n",
    "Given a function $y = f(x)$ (of any complexity),\n",
    "where $x$ and $y$ can be respectively the input and output tensors and $f$ the series of operations applied to them,\n",
    "we can compute the derivative $\\frac{dy}{dx}$ by:\n",
    "\n",
    "\n",
    "1. first computing the **forward pass** to have an actual scalar value of $y$,\n",
    "2. calling the method `y.backward()`,\n",
    "3. accessing the gradient value in the field `x.grad`.\n",
    "\n",
    "Remember to set the attribute `requires_grad=True` whenever you need to retreive gradients from a tensor. PyTorch will compute gradients only when specified to keep the code efficient.\n",
    "\n",
    "\n",
    "Let's see this procedure in practice!\n"
   ],
   "metadata": {
    "id": "uSGtC0b7WX3B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "print(x.grad)\n",
    "y = x**2    # forward pass: here we define and use the function\n",
    "print(y)\n",
    "y.backward()    # backward pass\n",
    "print(x.grad)"
   ],
   "metadata": {
    "id": "UoLzsO0kJ0wa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "296a1634-3977-499f-c94d-7f9bdef022bd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(6.)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "What if we run multiple times the following cell?\n",
    "Why the gradient is no more equal to the correct one?"
   ],
   "metadata": {
    "id": "8IT-xdaThKEY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y = x**2\n",
    "y.backward()\n",
    "print(x.grad)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-wfLAVEhJcj",
    "outputId": "d5e32c77-ba86-432f-c49c-6d4223f9eb3c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember that every time we call forward and backward, the gradients are not overwritten but accumulated.\n",
    "\n",
    "This is an important property needed for complex operations, but if disregarded can lead to wrong results.\n",
    "\n",
    "To fix this issue is sufficient to call the `.zero_()` method on the grad attribute\n"
   ],
   "metadata": {
    "id": "L096z3JXgLH9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y = x**2\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "x.grad.zero_()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guAOA4YdgKoF",
    "outputId": "aec16f98-2ab0-4587-fed1-189cc0bca4f8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(6.)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the function is more complex, such as a nested one, the code for computing the gradients does not change as PyTorch take care of implementing the chain rule automatically for computing derivatives all around the computational graph!"
   ],
   "metadata": {
    "id": "nWxUwnRZpteK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor(3., requires_grad=True)\n",
    "print(x.grad)\n",
    "y = (x**2).log()    # forward (only part of the code that changed)\n",
    "print(y)\n",
    "y.backward()    # backward\n",
    "print(x.grad)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ropeCuGzqCeY",
    "outputId": "efcb893d-77ec-40c0-e7a0-24dc655daf75"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "tensor(2.1972, grad_fn=<LogBackward0>)\n",
      "tensor(0.6667)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.2\n",
    "Consider the function $f = log(w \\cdot x + b)$.\n",
    "\n",
    "1. Compute the forward pass setting $x=2$, $w=0.5$ and $b=1.5$ and print the intermediate and output tensors.\n",
    "\n",
    "\n",
    "2. Compute the backward pass and print the gradients $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial w}$ and $\\frac{\\partial f}{\\partial b}$."
   ],
   "metadata": {
    "id": "FNl22py6xjDA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################\n",
    "# 1. Forward pass\n",
    "###########################################\n",
    "# TODO"
   ],
   "metadata": {
    "id": "0tQ-sw1X6INA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################\n",
    "# 1. Backward pass\n",
    "###########################################\n",
    "# TODO"
   ],
   "metadata": {
    "id": "yEPxiXAS8zG7"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
