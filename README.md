# Introductory Seminar of PyTorch for Deep Learning


In this repository you can find the material (progressively added) for the seminar given in the context of the Cagliari Digital Lab 2024 project.
If you want to register please fill out [this form](https://docs.google.com/forms/d/1KRKrZ7qyI7oM8MGf0hSsAV5nSv1QAVViqIVpeSm4Kkc/edit)

# Dates
| Date | Time    | Topic    | Room | Building |
| :---:   | :---: | :---: | :---: | :---: |
| 01/07/2024 | 9:00 - 13:00   | PyTorch Fundamentals  | Laboratorio L.I.D.I.A Software | Building N - Entrance N1 |
| 02/07/2024 | 9:00 - 13:00   | Implementation and Training of a Neural Network   | Laboratorio L.I.D.I.A Software | Building N - Entrance N1 |
| 03/07/2024 | 9:00 - 13:00   | Deep Learning for Computer Vision Tasks with PyTorch   | Laboratorio L.I.D.I.A Software | Building N - Entrance N1 |
| 04/07/2024 | 9:00 - 13:00   | Deep Learning for Natural Language Processing with PyTorch  | Laboratorio L.I.D.I.A Software | Building N - Entrance N1 |
| 05/07/2024 | 9:00 - 13:00   | Adapting to different frameworks: Tensorflow  | Laboratorio L.I.D.I.A Software | Building N - Entrance N1 |


# Program

:pushpin:**PyTorch Fundamentals**

In the first part, we will learn how to represent data as PyTorch tensor objects, how to manipulate this data structure, and how the autograd mechanism allows to automatically keep track of the tensor computations in order to obtain the gradients with respect to any variable.

:pushpin:**Implementation and Training of a Neural Network**

During the second part, we will learn how to implement a neural network in PyTorch, define the loss function, implement the gradient descent algorithm, and eventually evaluate our network. We will also see how we can use different predefined PyTorch modules, such as particular loss functions and optimizers, to obtain similar results with fewer lines of code.

:pushpin:**Deep Learning for Computer Vision Tasks with PyTorch**

The third part will be dedicated to deep learning techniques for computer vision, with a particular focus on image classification tasks. We will first see how visual data is represented as PyTorch tensors, how it can be manipulated and processed to improve convergence in deep neural networks and how we can use the PyTorch data loading mechanism to deal with large datasets efficiently. Then, we will focus on implementing a convolutional neural network (CNN): we will start from the concepts of convolution and pooling, batch normalization, regularization techniques and some more advanced architectures such as ResNet. Lastly, we will see how to finetune a pre-trained CNN for different classification tasks.

:pushpin:**Deep Learning for Natural Language Processing with PyTorch**

The fourth part will be dedicated to deep learning techniques for natural language processing tasks. We will first see how to deal with sequential data such as text by using word embeddings; then we will focus on the implementation of a particular class of neural networks: the recurrent neural networks (RNN). We will implement an RNN and we will explore the concept of back-propagation through time to train it in the context of text generation and sentiment analysis. Lastly, we can use and adapt a pre-trained language model to improve the performances on the latter downstream tasks.

:pushpin:**Adapting to different frameworks: Tensorflow**
The last part is dedicated to understanding how the learned fundamental concepts of deep learning can be easily transfered to different deep learning frameworks. To this end, we will focus on Tensorflow, another popular deep learning library. Here we will see how to reproduce some of the computer vision and NLP tasks that we learned with PyTorch.
